import httpx
from typing import Literal, Optional
from app.config import settings


class OllamaClient:
    """Local LLM chat using Ollama (RAG-friendly)"""

    def __init__(self):
        self.base_url = settings.OLLAMA_BASE_URL.rstrip("/")
        self.model = settings.OLLAMA_MODEL

        self.smalltalk = {
            "selam", "merhaba", "hello", "hi", "naber", "nasÄ±lsÄ±n",
            "teÅŸekkÃ¼rler", "saÄŸol", "hey", "mrb", "slm"
        }

        # Context limitleri (dÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - hÄ±z iÃ§in)
        self.max_ctx_chars_chat = 4000
        self.max_ctx_chars_summary_short = 6000
        self.max_ctx_chars_summary_long = 10000

        # Timeoutlar (artÄ±rÄ±ldÄ±)
        self.timeout_chat = httpx.Timeout(connect=10.0, read=600.0, write=600.0, pool=10.0)
        self.timeout_summary = httpx.Timeout(connect=10.0, read=600.0, write=600.0, pool=10.0)

    def _truncate(self, text: str, limit: int) -> str:
        if not text:
            return ""
        text = text.strip()
        return text if len(text) <= limit else text[:limit] + "\n\n[...kÄ±saltÄ±ldÄ±...]"

    def _parse_ollama_response(self, data: dict) -> str:
        # Ollama /api/generate -> {"response": "..."}
        if isinstance(data, dict) and "response" in data and isinstance(data["response"], str):
            return data["response"].strip()

        # BazÄ± wrapperâ€™lar / farklÄ± formatlar
        if isinstance(data, dict) and "message" in data:
            msg = data["message"]
            if isinstance(msg, dict) and isinstance(msg.get("content"), str):
                return msg["content"].strip()

        return ""

    async def generate_answer(
        self,
        question: str,
        context: str,
        system_prompt: Optional[str] = None,
        sources_hint: Optional[str] = None,  # istersen â€œKaynaklar: Page 5...â€ gibi eklersin
    ) -> str:
        """
        Chat/Q&A: Belge sorularÄ±nda sadece context'e dayanÄ±r.
        SelamlaÅŸma vb. kÃ¼Ã§Ã¼k konuÅŸmayÄ± sadece context YOKSA serbest bÄ±rakÄ±r.
        """

        q = (question or "").strip()
        q_lower = q.lower()
        ctx = (context or "").strip()

        # âœ… SelamlaÅŸma istisnasÄ±: SADECE context boÅŸken devreye girsin
        if (not ctx) and (q_lower in self.smalltalk):
            return "Merhaba!  Belgeyle ilgili bir soru sorarsan yÃ¼klediÄŸin iÃ§erikten yanÄ±tlayabilirim."

        # Context Ã§ok uzunsa kÄ±rp
        ctx = self._truncate(ctx, self.max_ctx_chars_chat)

        if system_prompt is None:
            system_prompt = """
Sen DocuMind asistanÄ±sÄ±n.
Kural 1: KullanÄ±cÄ±nÄ±n sorusu belgeyle ilgiliyse SADECE verilen BaÄŸlam'a dayanarak cevap ver.
Kural 2: BaÄŸlam yetersizse ÅŸu cÃ¼mleyi kullan: "Bu soruya verilen belgeler Ã¼zerinden cevap veremiyorum."
Kural 3: CevabÄ± kÄ±sa, net ve gÃ¶rev-odaklÄ± yaz. Cevap dili sorunun diliyle aynÄ± olsun.
Kural 4: CevabÄ±n sonunda MUTLAKA hangi kaynaÄŸÄ± kullandÄ±ÄŸÄ±nÄ± belirt. Format: "ðŸ“„ Kaynak: [Belge adÄ±], [Konum]"
"""

        # KaynaklarÄ± prompt iÃ§ine eklemek istersen
        if sources_hint:
            sources_block = f"\nKullanÄ±labilir Kaynaklar:\n{sources_hint}\n"
        else:
            sources_block = ""

        full_prompt = f"""{system_prompt.strip()}

BaÄŸlam:
{ctx}

{sources_block}
Soru: {q}

Cevap (sonunda kaynak belirt):
"""


        # Debug
        print(f"[ollama] MODEL={self.model} CTX_LEN={len(ctx)} Q_LEN={len(q)}")

        try:
            async with httpx.AsyncClient(timeout=self.timeout_chat) as client:
                resp = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.3,
                            "top_p": 0.9,
                            "num_predict": 512,  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - hÄ±z iÃ§in
                        },
                    },
                )
                resp.raise_for_status()
                data = resp.json()
                text = self._parse_ollama_response(data)

                if not text:
                    raise RuntimeError(f"Unexpected Ollama response payload: {data}")

                return text

        except httpx.ConnectError:
            raise Exception("Ollama servisi Ã§alÄ±ÅŸmÄ±yor. Terminalde `ollama serve` aÃ§Ä±k mÄ±?")
        except httpx.TimeoutException:
            raise Exception("Ollama timeout. Model yavaÅŸ olabilir veya context Ã§ok uzundur.")
        except Exception as e:
            print(f"[ollama] Error: {repr(e)}")
            raise Exception(f"Answer generation failed: {str(e)}")

    async def generate_summary(
        self,
        content: str,
        mode: Literal["short", "long"] = "short",
        document_name: str = "",
    ) -> str:
        """Document summary: short/long"""

        text = (content or "").strip()
        if not text:
            return "Ã–zet oluÅŸturmak iÃ§in dokÃ¼man iÃ§eriÄŸi bulunamadÄ±."

        # Context limitleri mode'a gÃ¶re
        if mode == "short":
            text = self._truncate(text, self.max_ctx_chars_summary_short)
            num_predict = 512  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - hÄ±z iÃ§in
            temperature = 0.2
            system_prompt = f"""
Sen DocuMind Ã¶zetleyicisisin.
DokÃ¼man adÄ±: {document_name}

GÃ¶rev: KISA Ã¶zet Ã¼ret.
Format:
- 1 paragraf genel Ã¶zet
- ardÄ±ndan en Ã¶nemli 5 madde (â€¢ ile)
Kurallar:
- Sadece dokÃ¼man iÃ§eriÄŸine dayan
- Uydurma bilgi ekleme
- TÃ¼rkÃ§e yaz
"""
        else:
            text = self._truncate(text, self.max_ctx_chars_summary_long)
            num_predict = 1024  # DÃ¼ÅŸÃ¼rÃ¼ldÃ¼ - hÄ±z iÃ§in
            temperature = 0.2
            system_prompt = f"""
Sen DocuMind Ã¶zetleyicisisin.
DokÃ¼man adÄ±: {document_name}

GÃ¶rev: DETAYLI Ã¶zet Ã¼ret.
Format:
1) Genel BakÄ±ÅŸ
2) Ana Konular (alt baÅŸlÄ±klarla)
3) Ã–nemli Noktalar (madde listesi)
4) SonuÃ§
Kurallar:
- Sadece dokÃ¼man iÃ§eriÄŸine dayan
- Uydurma bilgi ekleme
- TÃ¼rkÃ§e yaz
"""

        full_prompt = f"""{system_prompt.strip()}

DokÃ¼man Ä°Ã§eriÄŸi:
{text}

Ã–zet:
"""

        print(f"[ollama] Summary mode={mode} CONTENT_LEN={len(text)} MODEL={self.model}")

        try:
            async with httpx.AsyncClient(timeout=self.timeout_summary) as client:
                resp = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "top_p": 0.9,
                            "num_predict": num_predict,
                        },
                    },
                )
                resp.raise_for_status()
                data = resp.json()
                out = self._parse_ollama_response(data)

                if not out:
                    raise RuntimeError(f"Unexpected Ollama response payload: {data}")

                return out

        except httpx.ConnectError:
            raise Exception("Ollama servisi Ã§alÄ±ÅŸmÄ±yor. `ollama serve` aÃ§Ä±k mÄ±?")
        except httpx.TimeoutException:
            raise Exception("Ollama timeout. Ã–zet iÃ§in iÃ§erik Ã§ok uzun olabilir.")
        except Exception as e:
            print(f"[ollama] Summary error: {repr(e)}")
            raise Exception(f"Summary generation failed: {str(e)}")

    async def check_health(self) -> bool:
        """Check if Ollama is running"""
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
                r = await client.get(f"{self.base_url}/api/tags")
                return r.status_code == 200
        except Exception:
            return False


ollama_client = OllamaClient()
